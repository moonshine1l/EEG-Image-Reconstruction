{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "480e9dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing exp_paths: 100%|██████████| 31/31 [00:00<00:00, 153.96it/s]\n",
      "Loading .fif files: 100%|██████████| 434/434 [00:19<00:00, 21.73it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "if 'Modeling' in os.path.abspath(\"\").split('/'):\n",
    "    os.chdir('..')\n",
    "if 'Notebooks' in os.path.abspath(\"\").split('/'):\n",
    "    os.chdir('..')\n",
    "\n",
    "project_root = os.path.abspath(\"\")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "# Firstly import the class of dataset\n",
    "from Scripts.Data_Loader import EIRDataset\n",
    "\n",
    "EIR_Dataset = EIRDataset('./Generated/Data_Train/', task_type='geometric', n_jobs=72) # task type can be `geometric` or `random` or `all`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12a2ab1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_df(EIR_Dataset: EIRDataset, freq: int):\n",
    "    for i in range(len(EIR_Dataset)): \n",
    "        eeg_sample, eye_sample, metadata, label, img = EIR_Dataset[i]\n",
    "        eeg_sample.resample(freq)\n",
    "resample_df(EIR_Dataset, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3e073c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "loaded = np.load(project_root + '/Generated/Spectrums/exec_morlets.npz')\n",
    "\n",
    "results_arr = []\n",
    "i = 0\n",
    "while f'power_{i}' in loaded:\n",
    "    power = loaded[f'power_{i}']\n",
    "    phase = loaded[f'phase_{i}']\n",
    "    s_id = int(loaded[f'subject_id_{i}'])\n",
    "    t_id = int(loaded[f'trial_id_{i}'])\n",
    "    gender = str(loaded[f'gender_{i}'])\n",
    "    handiness = str(loaded[f'handiness_{i}'])\n",
    "    age = int(loaded[f'age_{i}'])\n",
    "    label = int(loaded[f'label_{i}'])\n",
    "    img = loaded[f'img_{i}']\n",
    "    task_type = str(loaded[f'task_type_{i}'])\n",
    "    \n",
    "    results_arr.append([power, phase, s_id, t_id, gender, handiness, age, label, img, task_type])\n",
    "    i += 1\n",
    "\n",
    "power, phase, s_id, t_id, gender, handiness, age, label, img, task_type = results_arr[0]\n",
    "del loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac0e3a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_wav = []\n",
    "phase_wav = []\n",
    "subj = []\n",
    "trial = []\n",
    "label_wav = []\n",
    "tasks = []\n",
    "\n",
    "max_len = 309\n",
    "\n",
    "for i, sample in enumerate(results_arr):\n",
    "\n",
    "    if i == len(EIR_Dataset):\n",
    "        break\n",
    "    power_wav.append(sample[0][:, :, :max_len])\n",
    "    phase_wav.append(sample[1][:, :, :max_len])\n",
    "    subj.append(sample[2])\n",
    "    trial.append(sample[3])\n",
    "    label_wav.append(sample[7])\n",
    "    \n",
    "power_wav = np.array(power_wav)\n",
    "phase_wav = np.array(phase_wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5a994d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_wav_combined = power_wav.transpose(0, 2, 1, 3).reshape(power_wav.shape[0], power_wav.shape[1], power_wav.shape[2]*power_wav.shape[3])\n",
    "phase_wav_combined = phase_wav.transpose(0, 2, 1, 3).reshape(phase_wav.shape[0], phase_wav.shape[1], phase_wav.shape[2]*phase_wav.shape[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "348f7a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from pyriemann.estimation import XdawnCovariances\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from pyriemann.estimation import Covariances\n",
    "from typing import Union\n",
    "\n",
    "n_components = 4\n",
    "class ModelCovariance:\n",
    "    class ParallelCovariances(BaseEstimator, TransformerMixin):\n",
    "        def __init__(self, n_filters, covariance_type: Union[list, type] = Covariances):\n",
    "            self.covariance_type = covariance_type\n",
    "            self.n_filters = n_filters\n",
    "            self.cov = []\n",
    "            self.cov_pipeline = []\n",
    "\n",
    "            for i, cov_type in enumerate(covariance_type):\n",
    "                if cov_type == XdawnCovariances:\n",
    "                    self.cov.append(cov_type(nfilter = self.n_filters[i], estimator = 'lwf', xdawn_estimator='lwf'))\n",
    "                else: self.cov.append(cov_type(estimator = 'lwf'))\n",
    "        def fit(self, X, y):\n",
    "            self.cov_pipeline = []\n",
    "            for i in range(len(X)):\n",
    "                self.cov_pipeline.append(Pipeline([\n",
    "                ('covariance', self.cov[i]),\n",
    "                ('tangent', TangentSpace())]))\n",
    "                self.cov_pipeline[i].fit(X[i], y)\n",
    "            return self\n",
    "        \n",
    "        def transform(self, X):\n",
    "            ret = []\n",
    "            for i in range(len(X)):\n",
    "                ret.append(self.cov_pipeline[i].transform(X[i]))\n",
    "            combined = np.concatenate(ret, axis=1)\n",
    "            return combined\n",
    "\n",
    "        \n",
    "    def __init__(self, feature_groups: int = 3, n_filters: Union[list, int, None] = 4, covariances: Union[list, type] = XdawnCovariances,\n",
    "                  Classifier: Union[list, BaseEstimator] = LogisticRegression(C=3, class_weight='balanced', max_iter = 300)):\n",
    "        self.feature_groups = feature_groups\n",
    "        self.covariances = covariances\n",
    "        self.n_filters = n_filters\n",
    "        if feature_groups == 1: # Предполагается что в этом случае нигде не используется list для передачи параметров\n",
    "            cov = covariances(estimator='lwf')\n",
    "            if covariances == XdawnCovariances:\n",
    "                cov = covariances(n_filters, estimator='lwf', xdawn_estimator='lwf')\n",
    "            self.pipeline = Pipeline([\n",
    "                ('cov', cov),\n",
    "                ('tang', TangentSpace()),\n",
    "                ('clf', Classifier)])\n",
    "        else: # Если используем листы для передачи параметров - следим чтобы длина совпадала с feature_groups\n",
    "            if isinstance(n_filters, int):\n",
    "                self.n_filters = [n_filters] * feature_groups\n",
    "            if isinstance(covariances, type):\n",
    "                self.covariances = [covariances] * feature_groups\n",
    "            self.pipeline = (Pipeline([\n",
    "                ('cov', self.ParallelCovariances(self.n_filters, self.covariances)),\n",
    "                ('clf', Classifier)]))\n",
    "    def get_pipeline(self):\n",
    "        return self.pipeline\n",
    "    def fit(self, X, y):\n",
    "        self.pipeline.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.pipeline.predict(X)\n",
    "    \n",
    "    def evaluate(self, X, true_y, metric: Union[str, list] = 'accuracy'):\n",
    "        preds = self.pipeline.predict(X)\n",
    "        if isinstance(metric, str):\n",
    "            if metric == 'accuracy':\n",
    "                return metrics.accuracy_score(true_y, preds)\n",
    "            elif metric == 'precision':\n",
    "                return metrics.precision_score(true_y, preds, average='weighted')\n",
    "            elif metric == 'recall':\n",
    "                return metrics.recall_score(true_y, preds, average='weighted')\n",
    "            elif metric == 'f1':\n",
    "                return metrics.f1_score(true_y, preds, average='weighted')\n",
    "            elif metric == 'roc_auc':\n",
    "                return metrics.roc_auc_score(true_y, preds)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown metric: {metric}\")\n",
    "        \n",
    "        elif isinstance(metric, list):\n",
    "            results = {}\n",
    "            for m in metric:\n",
    "                if m == 'accuracy':\n",
    "                    results[m] = metrics.accuracy_score(true_y, preds)\n",
    "                elif m == 'precision':\n",
    "                    results[m] = metrics.precision_score(true_y, preds, average='weighted')\n",
    "                elif m == 'recall':\n",
    "                    results[m] = metrics.recall_score(true_y, preds, average='weighted')\n",
    "                elif m == 'f1':\n",
    "                    results[m] = metrics.f1_score(true_y, preds, average='weighted')\n",
    "                elif m == 'roc_auc':\n",
    "                    results[m] = metrics.roc_auc_score(true_y, preds)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown metric: {m}\")\n",
    "            return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138111fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject ID: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tN Splits: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [02:59<08:57, 179.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tN Splits: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [08:19<00:00, 125.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tN Splits: 7\n",
      "ERROR AT: n_splits=7.\n",
      "ERROR: n_splits=7 cannot be greater than the number of members in each class.\n",
      "\tN Splits: 9\n",
      "ERROR AT: n_splits=9.\n",
      "ERROR: n_splits=9 cannot be greater than the number of members in each class.\n",
      "Subject ID: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tN Splits: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [02:45<08:15, 165.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tN Splits: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [08:11<08:40, 260.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tN Splits: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [16:05<00:00, 241.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tN Splits: 9\n",
      "ERROR AT: n_splits=9.\n",
      "ERROR: n_splits=9 cannot be greater than the number of members in each class.\n",
      "Subject ID: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tN Splits: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [02:21<07:04, 141.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tN Splits: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [06:46<07:08, 214.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tN Splits: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [13:22<00:00, 200.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tN Splits: 9\n",
      "ERROR AT: n_splits=9.\n",
      "ERROR: n_splits=9 cannot be greater than the number of members in each class.\n",
      "Subject ID: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tN Splits: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [02:38<07:56, 158.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tN Splits: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [07:29<00:00, 112.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tN Splits: 7\n",
      "ERROR AT: n_splits=7.\n",
      "ERROR: n_splits=7 cannot be greater than the number of members in each class.\n",
      "\tN Splits: 9\n",
      "ERROR AT: n_splits=9.\n",
      "ERROR: n_splits=9 cannot be greater than the number of members in each class.\n",
      "Subject ID: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tN Splits: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [02:20<07:02, 140.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tN Splits: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [06:42<00:00, 100.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tN Splits: 7\n",
      "ERROR AT: n_splits=7.\n",
      "ERROR: n_splits=7 cannot be greater than the number of members in each class.\n",
      "\tN Splits: 9\n",
      "ERROR AT: n_splits=9.\n",
      "ERROR: n_splits=9 cannot be greater than the number of members in each class.\n",
      "Subject ID: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tN Splits: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [02:41<08:03, 161.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tN Splits: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [07:11<00:00, 107.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tN Splits: 7\n",
      "ERROR AT: n_splits=7.\n",
      "ERROR: n_splits=7 cannot be greater than the number of members in each class.\n",
      "\tN Splits: 9\n",
      "ERROR AT: n_splits=9.\n",
      "ERROR: n_splits=9 cannot be greater than the number of members in each class.\n",
      "Subject ID: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tN Splits: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [02:23<07:09, 143.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tN Splits: 5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from itertools import product\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "from Scripts import Selectors_From_Dataset as sel\n",
    "\n",
    "splits = [3, 5, 7, 9]\n",
    "\n",
    "results_storage = {\n",
    "    1: [],\n",
    "    2: [],\n",
    "    3: [],\n",
    "    4: [],\n",
    "    5: []\n",
    "}\n",
    "\n",
    "metrics_list = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "\n",
    "models = {\n",
    "                1: ModelCovariance(feature_groups=1, n_filters=4,\n",
    "                                    covariances=XdawnCovariances,\n",
    "                                    Classifier=LogisticRegression(C=3, class_weight='balanced', max_iter=300)),\n",
    "                2: ModelCovariance(feature_groups=3, n_filters=4,\n",
    "                                    covariances=[XdawnCovariances, Covariances, Covariances],\n",
    "                                    Classifier=LogisticRegression(C=3, class_weight='balanced', max_iter=300)),\n",
    "                3: ModelCovariance(feature_groups=2, n_filters=4,\n",
    "                                    covariances=[XdawnCovariances, Covariances],\n",
    "                                    Classifier=LogisticRegression(C=3, class_weight='balanced', max_iter=300)),\n",
    "                4: ModelCovariance(feature_groups=2, n_filters=4,\n",
    "                                    covariances=[XdawnCovariances, Covariances],\n",
    "                                    Classifier=LogisticRegression(C=3, class_weight='balanced', max_iter=300)),\n",
    "                5: ModelCovariance(feature_groups=2, n_filters=4,\n",
    "                                    covariances=[Covariances, Covariances],\n",
    "                                    Classifier=LogisticRegression(C=3, class_weight='balanced', max_iter=300)),\n",
    "            }\n",
    "\n",
    "s_ids = np.unique(np.array([result[2] for result in results_arr]))\n",
    "\n",
    "CUT_IDS = False\n",
    "if CUT_IDS:\n",
    "    s_ids = s_ids[:3]\n",
    "\n",
    "for s_id in s_ids:\n",
    "    print(f\"Subject ID: {s_id}\")\n",
    "    X, img, y = sel.get_sample(EIR_Dataset, [s_id])\n",
    "\n",
    "    for n_splits in tqdm(splits):\n",
    "        try:\n",
    "            k_fold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "            for train_index, test_index in k_fold.split(X, y):\n",
    "                X_train, X_test = X[train_index], X[test_index]\n",
    "                Y_train, Y_test = y[train_index], y[test_index]\n",
    "                power_wav_train, power_wav_test = power_wav_combined[train_index], power_wav_combined[test_index]\n",
    "                phase_wav_train, phase_wav_test = phase_wav_combined[train_index], phase_wav_combined[test_index]\n",
    "                y_wav_train, y_wav_test = np.array(label_wav)[train_index], np.array(label_wav)[test_index]\n",
    "\n",
    "                inputs_train = {\n",
    "                    1: X_train,\n",
    "                    2: [X_train, power_wav_train, phase_wav_train],\n",
    "                    3: [X_train, power_wav_train],\n",
    "                    4: [X_train, phase_wav_train],\n",
    "                    5: [power_wav_train, phase_wav_train]\n",
    "                }\n",
    "\n",
    "                inputs_test = {\n",
    "                    1: X_test,\n",
    "                    2: [X_test, power_wav_test, phase_wav_test],\n",
    "                    3: [X_test, power_wav_test],\n",
    "                    4: [X_test, phase_wav_test],\n",
    "                    5: [power_wav_test, phase_wav_test]\n",
    "                }\n",
    "\n",
    "                # Обучение и оценка моделей\n",
    "                for i in models:\n",
    "                    models[i].fit(inputs_train[i], Y_train)\n",
    "                    results = models[i].evaluate(inputs_test[i], Y_test, metrics_list)\n",
    "                    \n",
    "                    storage_item = {\n",
    "                        'n_splits': n_splits,\n",
    "                        'fold': len(results_storage[i]) % n_splits + 1}\n",
    "                    for metric in metrics_list:\n",
    "                        storage_item[metric] = results[metric]\n",
    "                        \n",
    "                    results_storage[i].append(storage_item)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR AT: n_splits={n_splits}.\\nERROR: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300f31ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "output_path = './Generated/Results/per_subject_results.json'\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump({\n",
    "        'models': {i: f\"Model_{i}\" for i in models},\n",
    "        'metrics': metrics_list,\n",
    "        'data_size': len(X),\n",
    "        'results': results_storage\n",
    "    }, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d69be1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import product\n",
    "from sklearn.model_selection import LeavePGroupsOut\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "from Scripts import Selectors_From_Dataset as sel\n",
    "\n",
    "groups_out_variants = [1, 2, 3]\n",
    "\n",
    "results_storage = {\n",
    "    1: [],\n",
    "    2: [],\n",
    "    3: [],\n",
    "    4: [],\n",
    "    5: []\n",
    "}\n",
    "\n",
    "metrics_list = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "\n",
    "models = {\n",
    "                1: ModelCovariance(feature_groups=1, n_filters=4,\n",
    "                                    covariances=XdawnCovariances,\n",
    "                                    Classifier=LogisticRegression(C=3, class_weight='balanced', max_iter=300)),\n",
    "                2: ModelCovariance(feature_groups=3, n_filters=4,\n",
    "                                    covariances=[XdawnCovariances, Covariances, Covariances],\n",
    "                                    Classifier=LogisticRegression(C=3, class_weight='balanced', max_iter=300)),\n",
    "                3: ModelCovariance(feature_groups=2, n_filters=4,\n",
    "                                    covariances=[XdawnCovariances, Covariances],\n",
    "                                    Classifier=LogisticRegression(C=3, class_weight='balanced', max_iter=300)),\n",
    "                4: ModelCovariance(feature_groups=2, n_filters=4,\n",
    "                                    covariances=[XdawnCovariances, Covariances],\n",
    "                                    Classifier=LogisticRegression(C=3, class_weight='balanced', max_iter=300)),\n",
    "                5: ModelCovariance(feature_groups=2, n_filters=4,\n",
    "                                    covariances=[Covariances, Covariances],\n",
    "                                    Classifier=LogisticRegression(C=3, class_weight='balanced', max_iter=300)),\n",
    "            }\n",
    "\n",
    "\n",
    "s_ids = np.unique(np.array([result[2] for result in results_arr]))\n",
    "\n",
    "CUT_IDS = False\n",
    "if CUT_IDS:\n",
    "    s_ids = s_ids[:3]\n",
    "\n",
    "X, img, y = sel.get_sample(EIR_Dataset, [s_ids])\n",
    "\n",
    "for groups_out in groups_out_variants:\n",
    "    lpgo = LeavePGroupsOut(n_groups=groups_out)\n",
    "    for train_index, test_index in tqdm(lpgo.split(X, y, groups=subj), total=len(s_ids), desc=\"LPGO\"):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test = y[train_index], y[test_index]\n",
    "        power_wav_train, power_wav_test = power_wav_combined[train_index], power_wav_combined[test_index]\n",
    "        phase_wav_train, phase_wav_test = phase_wav_combined[train_index], phase_wav_combined[test_index]\n",
    "        y_wav_train, y_wav_test = np.array(label_wav)[train_index], np.array(label_wav)[test_index]\n",
    "\n",
    "        inputs_train = {\n",
    "            1: X_train,\n",
    "            2: [X_train, power_wav_train, phase_wav_train],\n",
    "            3: [X_train, power_wav_train],\n",
    "            4: [X_train, phase_wav_train],\n",
    "            5: [power_wav_train, phase_wav_train]\n",
    "        }\n",
    "\n",
    "        inputs_test = {\n",
    "            1: X_test,\n",
    "            2: [X_test, power_wav_test, phase_wav_test],\n",
    "            3: [X_test, power_wav_test],\n",
    "            4: [X_test, phase_wav_test],\n",
    "            5: [power_wav_test, phase_wav_test]\n",
    "        }\n",
    "\n",
    "        test_subject = int(subj[test_index][0])\n",
    "        # Обучение и оценка моделей\n",
    "        for i in models:\n",
    "            try:\n",
    "                models[i].fit(inputs_train[i], Y_train)\n",
    "                results = models[i].evaluate(inputs_test[i], Y_test, metrics_list)\n",
    "                storage_item = {\n",
    "                    'test_subject_id': test_subject,\n",
    "                    'subjects_in_validation': groups_out\n",
    "                }\n",
    "                for metric in metrics_list:\n",
    "                    storage_item[metric] = results.get(metric, 0.0)\n",
    "                results_storage[i].append(storage_item)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR AT: n_splits={n_splits}.\\nERROR: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b5192d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "output_path = './Generated/Results/cross_subject_results.json'\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump({\n",
    "        'models': {i: f\"Model_{i}\" for i in models},\n",
    "        'metrics': metrics_list,\n",
    "        'data_size': len(X),\n",
    "        'results': results_storage\n",
    "    }, f, indent=4, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
